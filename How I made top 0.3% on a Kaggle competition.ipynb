{"cells":[{"metadata":{},"cell_type":"markdown","source":"# How I made top 0.3% on a Kaggle competition\nGetting started with competitive data science can be quite intimidating. So I wrote this quick overview of how I made top 0.3% on the Advanced Regression Techniques competition. If there is interest, Iâ€™m happy to do deep dives into the intuition behind the feature engineering and models used in this kernel.\n\nI encourage you to fork this kernel, play with the code and enter the competition. Good luck!\n\nIf you like this kernel, please give it an upvote. Thank you!\n\n## The Goal\n\n- Each row in the dataset describes the characteristics of a house.\n- Our goal is to predict the SalePrice, given these features.\n- Our models are evaluated on the Root-Mean-Squared-Error (RMSE) between the log of the SalePrice predicted by our model, and the log of the actual SalePrice. Converting RMSE errors to a log scale ensures that errors in predicting expensive houses and cheap houses will affect our score equally.\n\n## Key features of the model training process in this kernel:\n- **Cross Validation:** Using 12-fold cross-validation\n- **Models:** On each run of cross-validation I fit 7 models (ridge, svr, gradient boosting, random forest, xgboost, lightgbm regressors)\n- **Stacking:** In addition, I trained a meta StackingCVRegressor optimized using xgboost\n- **Blending:** All models trained will overfit the training data to varying degrees. Therefore, to make final predictions, I blended their predictions together to get more robust predictions."},{"metadata":{},"cell_type":"markdown","source":"## Model Performance\n\nWe can observe from the graph below that the blended model far outperforms the other models, with an RMSLE of 0.075. This is the model I used for making the final predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(\"../input/kernel-files/model_training_advanced_regression.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now that we have some context, let's get started!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Essentials\nimport numpy as np\nimport pandas as pd\nimport datetime\nimport random\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Models\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\n# Stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n# Misc\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\n\npd.set_option('display.max_columns', None)\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000\n\nimport os\nprint(os.listdir(\"../input/kernel-files\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Read in the dataset as a dataframe\ntrain = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{},"cell_type":"markdown","source":"## The Goal\n\n- Each row in the dataset describes the characteristics of a house.\n- Our goal is to predict the SalePrice, given these features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preview the data we're working with\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SalePrice: the variable we're trying to predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.distplot(train['SalePrice'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Skew and kurt\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features: a deep dive"},{"metadata":{},"cell_type":"markdown","source":"Let's visualize some of the features in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding numeric features\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in train.columns:\n    if train[i].dtype in numeric_dtypes:\n        if i in ['TotalSF', 'Total_Bathrooms','Total_porch_sf','haspool','hasgarage','hasbsmt','hasfireplace']:\n            pass\n        else:\n            numeric.append(i)     \n# visualising some more outliers in the data values\nfig, axs = plt.subplots(ncols=2, nrows=0, figsize=(12, 120))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\nsns.color_palette(\"husl\", 8)\nfor i, feature in enumerate(list(train[numeric]), 1):\n    if(feature=='MiscVal'):\n        break\n    plt.subplot(len(list(numeric)), 3, i)\n    sns.scatterplot(x=feature, y='SalePrice', hue='SalePrice', palette='Blues', data=train)\n        \n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(loc='best', prop={'size': 10})\n        \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"and plot how the features are correlated to each other, and to SalePrice"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = train.corr()\nplt.subplots(figsize=(15,12))\nsns.heatmap(corr, vmax=0.9, cmap=\"Blues\", square=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot how SalePrice relates to some of the features in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([train['SalePrice'], train['OverallQual']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=train['OverallQual'], y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([train['SalePrice'], train['YearBuilt']], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=train['YearBuilt'], y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=45);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([train['SalePrice'], train['TotalBsmtSF']], axis=1)\ndata.plot.scatter(x='TotalBsmtSF', y='SalePrice', alpha=0.3, ylim=(0,800000));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([train['SalePrice'], train['LotArea']], axis=1)\ndata.plot.scatter(x='LotArea', y='SalePrice', alpha=0.3, ylim=(0,800000));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([train['SalePrice'], train['GrLivArea']], axis=1)\ndata.plot.scatter(x='GrLivArea', y='SalePrice', alpha=0.3, ylim=(0,800000));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove the Ids from train and test, as they are unique for each row and hence not useful for the model\ntrain_ID = train['Id']\ntest_ID = test['Id']\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the distribution of the SalePrice."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.distplot(train['SalePrice'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The SalePrice is skewed to the right. This is a problem because most ML models don't do well with non-normally distributed data. We can apply a log(1+x) tranform to fix the skew."},{"metadata":{"trusted":true},"cell_type":"code","source":"# log(1+x) transform\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot the SalePrice again."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm, color=\"b\");\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The SalePrice is now normally distributed, excellent!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove outliers\ntrain.drop(train[(train['OverallQual']<5) & (train['SalePrice']>200000)].index, inplace=True)\ntrain.drop(train[(train['GrLivArea']>4500) & (train['SalePrice']<300000)].index, inplace=True)\ntrain.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split features and labels\ntrain_labels = train['SalePrice'].reset_index(drop=True)\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test\n\n# Combine train and test features in order to apply the feature transformation pipeline to the entire dataset\nall_features = pd.concat([train_features, test_features]).reset_index(drop=True)\nall_features.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fill missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# determine the threshold for missing values\ndef percent_missing(df):\n    data = pd.DataFrame(df)\n    df_cols = list(pd.DataFrame(data))\n    dict_x = {}\n    for i in range(0, len(df_cols)):\n        dict_x.update({df_cols[i]: round(data[df_cols[i]].isnull().mean()*100,2)})\n    \n    return dict_x\n\nmissing = percent_missing(all_features)\ndf_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\nprint('Percent of missing data')\ndf_miss[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize missing values\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nsns.set_color_codes(palette='deep')\nmissing = round(train.isnull().mean()*100,2)\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar(color=\"b\")\n# Tweak the visual presentation\nax.xaxis.grid(False)\nax.set(ylabel=\"Percent of missing values\")\nax.set(xlabel=\"Features\")\nax.set(title=\"Percent missing data by feature\")\nsns.despine(trim=True, left=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now move through each of the features above and impute the missing values for each of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some of the non-numeric predictors are stored as numbers; convert them into strings \nall_features['MSSubClass'] = all_features['MSSubClass'].apply(str)\nall_features['YrSold'] = all_features['YrSold'].astype(str)\nall_features['MoSold'] = all_features['MoSold'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def handle_missing(features):\n    # the data description states that NA refers to typical ('Typ') values\n    features['Functional'] = features['Functional'].fillna('Typ')\n    # Replace the missing values in each of the columns below with their mode\n    features['Electrical'] = features['Electrical'].fillna(\"SBrkr\")\n    features['KitchenQual'] = features['KitchenQual'].fillna(\"TA\")\n    features['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\n    features['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\n    features['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n    features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n    \n    # the data description stats that NA refers to \"No Pool\"\n    features[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")\n    # Replacing the missing values with 0, since no garage = no cars in garage\n    for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n        features[col] = features[col].fillna(0)\n    # Replacing the missing values with None\n    for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n        features[col] = features[col].fillna('None')\n    # NaN values for these categorical basement features, means there's no basement\n    for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n        features[col] = features[col].fillna('None')\n        \n    # Group the by neighborhoods, and fill in missing value by the median LotFrontage of the neighborhood\n    features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n    # We have no particular intuition around how to fill in the rest of the categorical features\n    # So we replace their missing values with None\n    objects = []\n    for i in features.columns:\n        if features[i].dtype == object:\n            objects.append(i)\n    features.update(features[objects].fillna('None'))\n        \n    # And we do the same thing for numerical features, but this time with 0s\n    numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    numeric = []\n    for i in features.columns:\n        if features[i].dtype in numeric_dtypes:\n            numeric.append(i)\n    features.update(features[numeric].fillna(0))    \n    return features\n\nall_features = handle_missing(all_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's make sure we handled all the missing values\nmissing = percent_missing(all_features)\ndf_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\nprint('Percent of missing data')\ndf_miss[0:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no missing values anymore!"},{"metadata":{},"cell_type":"markdown","source":"## Fix skewed features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fetch all numeric features\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in all_features.columns:\n    if all_features[i].dtype in numeric_dtypes:\n        numeric.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create box plots for all numeric features\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=all_features[numeric] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find skewed numerical features\nskew_features = all_features[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use the scipy function boxcox1p which computes the Box-Cox transformation. The goal is to find a simple transformation that lets us normalize data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize skewed features\nfor i in skew_index:\n    all_features[i] = boxcox1p(all_features[i], boxcox_normmax(all_features[i] + 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's make sure we handled all the skewed values\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=all_features[skew_index] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the features look fairly normally distributed now."},{"metadata":{},"cell_type":"markdown","source":"## Create interesting features"},{"metadata":{},"cell_type":"markdown","source":"ML models have trouble recognizing more complex patterns (and we're staying away from neural nets for this competition), so let's help our models out by creating a few features based on our intuition about the dataset, e.g. total area of floors, bathrooms and porch area of each house."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features['BsmtFinType1_Unf'] = 1*(all_features['BsmtFinType1'] == 'Unf')\nall_features['HasWoodDeck'] = (all_features['WoodDeckSF'] == 0) * 1\nall_features['HasOpenPorch'] = (all_features['OpenPorchSF'] == 0) * 1\nall_features['HasEnclosedPorch'] = (all_features['EnclosedPorch'] == 0) * 1\nall_features['Has3SsnPorch'] = (all_features['3SsnPorch'] == 0) * 1\nall_features['HasScreenPorch'] = (all_features['ScreenPorch'] == 0) * 1\nall_features['YearsSinceRemodel'] = all_features['YrSold'].astype(int) - all_features['YearRemodAdd'].astype(int)\nall_features['Total_Home_Quality'] = all_features['OverallQual'] + all_features['OverallCond']\nall_features = all_features.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\nall_features['TotalSF'] = all_features['TotalBsmtSF'] + all_features['1stFlrSF'] + all_features['2ndFlrSF']\nall_features['YrBltAndRemod'] = all_features['YearBuilt'] + all_features['YearRemodAdd']\n\nall_features['Total_sqr_footage'] = (all_features['BsmtFinSF1'] + all_features['BsmtFinSF2'] +\n                                 all_features['1stFlrSF'] + all_features['2ndFlrSF'])\nall_features['Total_Bathrooms'] = (all_features['FullBath'] + (0.5 * all_features['HalfBath']) +\n                               all_features['BsmtFullBath'] + (0.5 * all_features['BsmtHalfBath']))\nall_features['Total_porch_sf'] = (all_features['OpenPorchSF'] + all_features['3SsnPorch'] +\n                              all_features['EnclosedPorch'] + all_features['ScreenPorch'] +\n                              all_features['WoodDeckSF'])\nall_features['TotalBsmtSF'] = all_features['TotalBsmtSF'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\nall_features['2ndFlrSF'] = all_features['2ndFlrSF'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\nall_features['GarageArea'] = all_features['GarageArea'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\nall_features['GarageCars'] = all_features['GarageCars'].apply(lambda x: 0 if x <= 0.0 else x)\nall_features['LotFrontage'] = all_features['LotFrontage'].apply(lambda x: np.exp(4.2) if x <= 0.0 else x)\nall_features['MasVnrArea'] = all_features['MasVnrArea'].apply(lambda x: np.exp(4) if x <= 0.0 else x)\nall_features['BsmtFinSF1'] = all_features['BsmtFinSF1'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\n\nall_features['haspool'] = all_features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nall_features['has2ndfloor'] = all_features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasgarage'] = all_features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasbsmt'] = all_features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasfireplace'] = all_features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature transformations\nLet's create more features by calculating the log and square transformations of our numerical features. We do this manually, because ML models won't be able to reliably tell if log(feature) or feature^2 is a predictor of the SalePrice."},{"metadata":{"trusted":true},"cell_type":"code","source":"def logs(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(np.log(1.01+res[l])).values)   \n        res.columns.values[m] = l + '_log'\n        m += 1\n    return res\n\nlog_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n                 'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                 'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',\n                 'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','YearRemodAdd','TotalSF']\n\nall_features = logs(all_features, log_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def squares(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(res[l]*res[l]).values)   \n        res.columns.values[m] = l + '_sq'\n        m += 1\n    return res \n\nsquared_features = ['YearRemodAdd', 'LotFrontage_log', \n              'TotalBsmtSF_log', '1stFlrSF_log', '2ndFlrSF_log', 'GrLivArea_log',\n              'GarageCars_log', 'GarageArea_log']\nall_features = squares(all_features, squared_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encode categorical features"},{"metadata":{},"cell_type":"markdown","source":"Numerically encode categorical features because most models can only handle numerical features."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features = pd.get_dummies(all_features).reset_index(drop=True)\nall_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove any duplicated column names\nall_features = all_features.loc[:,~all_features.columns.duplicated()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Recreate training and test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = all_features.iloc[:len(train_labels), :]\nX_test = all_features.iloc[len(train_labels):, :]\nX.shape, train_labels.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualize some of the features we're going to train our models on."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding numeric features\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in X.columns:\n    if X[i].dtype in numeric_dtypes:\n        if i in ['TotalSF', 'Total_Bathrooms','Total_porch_sf','haspool','hasgarage','hasbsmt','hasfireplace']:\n            pass\n        else:\n            numeric.append(i)     \n# visualising some more outliers in the data values\nfig, axs = plt.subplots(ncols=2, nrows=0, figsize=(12, 150))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\nsns.color_palette(\"husl\", 8)\nfor i, feature in enumerate(list(X[numeric]), 1):\n    if(feature=='MiscVal'):\n        break\n    plt.subplot(len(list(numeric)), 3, i)\n    sns.scatterplot(x=feature, y='SalePrice', hue='SalePrice', palette='Blues', data=train)\n        \n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(loc='best', prop={'size': 10})\n        \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train a model"},{"metadata":{},"cell_type":"markdown","source":"## Key features of the model training process:\n- **Cross Validation:** Using 12-fold cross-validation\n- **Models:** On each run of cross-validation I fit 7 models (ridge, svr, gradient boosting, random forest, xgboost, lightgbm regressors)\n- **Stacking:** In addition, I trained a meta StackingCVRegressor optimized using xgboost\n- **Blending:** All models trained will overfit the training data to varying degrees. Therefore, to make final predictions, I blended their predictions together to get more robust predictions."},{"metadata":{},"cell_type":"markdown","source":"## Setup cross validation and define error metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setup cross validation folds\nkf = KFold(n_splits=12, random_state=42, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define error metrics\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, train_labels, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setup models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Light Gradient Boosting Regressor\nlightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=7000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)\n\n# XGBoost Regressor\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=6000,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:linear',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\n# Ridge Regressor\nridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))\n\n# Support Vector Regressor\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n\n# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)  \n\n# Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=1200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=42)\n\n# Stack up all the models above, optimized using xgboost\nstack_gen = StackingCVRegressor(regressors=(xgboost, lightgbm, svr, ridge, gbr, rf),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train models"},{"metadata":{},"cell_type":"markdown","source":"### Get cross validation scores for each model"},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = {}\n\nscore = cv_rmse(lightgbm)\nprint(\"lightgbm: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['lgb'] = (score.mean(), score.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = cv_rmse(xgboost)\nprint(\"xgboost: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['xgb'] = (score.mean(), score.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = cv_rmse(svr)\nprint(\"SVR: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['svr'] = (score.mean(), score.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = cv_rmse(ridge)\nprint(\"ridge: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['ridge'] = (score.mean(), score.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = cv_rmse(rf)\nprint(\"rf: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['rf'] = (score.mean(), score.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = cv_rmse(gbr)\nprint(\"gbr: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['gbr'] = (score.mean(), score.std())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fit the models"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(X), np.array(train_labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('lightgbm')\nlgb_model_full_data = lightgbm.fit(X, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('xgboost')\nxgb_model_full_data = xgboost.fit(X, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Svr')\nsvr_model_full_data = svr.fit(X, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Ridge')\nridge_model_full_data = ridge.fit(X, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('RandomForest')\nrf_model_full_data = rf.fit(X, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('GradientBoosting')\ngbr_model_full_data = gbr.fit(X, train_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Blend models and get predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Blend models in order to make the final predictions more robust to overfitting\ndef blended_predictions(X):\n    return ((0.1 * ridge_model_full_data.predict(X)) + \\\n            (0.2 * svr_model_full_data.predict(X)) + \\\n            (0.1 * gbr_model_full_data.predict(X)) + \\\n            (0.1 * xgb_model_full_data.predict(X)) + \\\n            (0.1 * lgb_model_full_data.predict(X)) + \\\n            (0.05 * rf_model_full_data.predict(X)) + \\\n            (0.35 * stack_gen_model.predict(np.array(X))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get final precitions from the blended model\nblended_score = rmsle(train_labels, blended_predictions(X))\nscores['blended'] = (blended_score, 0)\nprint('RMSLE score on train data:')\nprint(blended_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Identify the best performing model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the predictions for each model\nsns.set_style(\"white\")\nfig = plt.figure(figsize=(24, 12))\n\nax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])\nfor i, score in enumerate(scores.values()):\n    ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\nplt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\nplt.xlabel('Model', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=13.5)\nplt.tick_params(axis='y', labelsize=12.5)\n\nplt.title('Scores of Models', size=20)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe from the graph above that the blended model far outperforms the other models, with an RMSLE of 0.075. This is the model I'll use for making the final predictions."},{"metadata":{},"cell_type":"markdown","source":"## Submit predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read in sample_submission dataframe\nsubmission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\nsubmission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Append predictions from blended models\nsubmission.iloc[:,1] = np.floor(np.expm1(blended_predictions(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fix outleir predictions\nq1 = submission['SalePrice'].quantile(0.0045)\nq2 = submission['SalePrice'].quantile(0.99)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\nsubmission.to_csv(\"submission_regression1.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale predictions\nsubmission['SalePrice'] *= 1.001619\nsubmission.to_csv(\"submission_regression2.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}